{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Title - Creating functions to compute various losses."
      ],
      "metadata": {
        "id": "NytCJnW5Kttg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning and deep learning, the training process involves optimizing a model's parameters to make accurate predictions on new data. Loss functions play a vital role in this optimization process by quantifying the difference between the model's predictions and the actual target values. The objective is to minimize the loss, which, in turn, improves the model's performance.\n",
        "\n",
        "L1 Loss (Mean Absolute Error):\n",
        "The L1 loss, also known as the Mean Absolute Error (MAE), measures the absolute difference between the predicted values and the true values. It is computed by taking the absolute difference between each prediction and target, and then averaging those differences. L1 loss is less sensitive to outliers compared to other loss functions like L2 loss. The L1 loss is defined as:\n",
        "\n",
        "L1 Loss Formula = L1Loss(y_true, y_pred) = (1/N) * Σ |y_true^(i) - y_pred^(i)|\n",
        "\n",
        "L2 Loss (Mean Squared Error):\n",
        "The L2 loss, also known as the Mean Squared Error (MSE), measures the squared difference between the predicted values and the true values. It is widely used in regression problems and is particularly sensitive to outliers. The L2 loss is defined as:\n",
        "\n",
        "L2 Loss Formula = L2Loss(y_true, y_pred) = (1/N) * Σ (y_true^(i) - y_pred^(i))^2\n",
        "\n",
        "Binary Cross Entropy:\n",
        "The binary cross entropy loss is used in binary classification problems, where the output can be either 0 or 1. It measures the dissimilarity between the true binary labels and the predicted probabilities for the positive class. It is defined as:\n",
        "\n",
        "Binary Cross Entropy Formula = BinaryCrossEntropy(y_true, y_pred) = - Σ [y_true^(i) * log(y_pred^(i)) + (1 - y_true^(i)) * log(1 - y_pred^(i))]\n",
        "\n",
        "Categorical Cross Entropy:\n",
        "The categorical cross entropy loss is used in multi-class classification problems. It measures the dissimilarity between the true class labels and the predicted probabilities for each class. Categorical cross entropy is commonly used with softmax activation in the output layer of the model. It is defined as:\n",
        "\n",
        "Categorical Cross Entropy Formula = CategoricalCrossEntropy(y_true, y_pred) = - Σ [y_true^(i) * log(y_pred^(i))]\n",
        "\n",
        "Where K is the number of classes in the classification problem."
      ],
      "metadata": {
        "id": "NS13GCsCKtl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "n23JTbBDHelm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def l1_loss(y_true, y_pred):\n",
        "  \"\"\"Computes the L1 loss between y_true and y_pred.\"\"\"\n",
        "  return np.sum(np.abs(y_true - y_pred))"
      ],
      "metadata": {
        "id": "RFex2S8PIg__"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def l2_loss(y_true, y_pred):\n",
        "  \"\"\"Computes the L2 loss between y_true and y_pred.\"\"\"\n",
        "  return np.sum((y_true - y_pred)**2)"
      ],
      "metadata": {
        "id": "bvIdyvfyIhDg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(y_true, y_pred):\n",
        "  \"\"\"Computes the binary cross entropy loss between y_true and y_pred.\"\"\"\n",
        "  return -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
      ],
      "metadata": {
        "id": "k7ZSKb4IJIOL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_cross_entropy(y_true, y_pred):\n",
        "  \"\"\"Computes the categorical cross entropy loss between y_true and y_pred.\"\"\"\n",
        "  return -np.sum(y_true * np.log(y_pred))"
      ],
      "metadata": {
        "id": "B3kIgGeUIhFx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create some data\n",
        "y_true = np.array([1, 0, 1])\n",
        "y_pred = np.array([0.9, 0.1, 0.9])"
      ],
      "metadata": {
        "id": "mYva1MbWIhIT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the losses\n",
        "l1_loss_value = l1_loss(y_true, y_pred)\n",
        "l2_loss_value = l2_loss(y_true, y_pred)\n",
        "binary_cross_entropy_value = binary_cross_entropy(y_true, y_pred)\n",
        "categorical_cross_entropy_value = categorical_cross_entropy(y_true, y_pred)"
      ],
      "metadata": {
        "id": "1YWLQEtUIhLG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the losses\n",
        "print(\"L1 loss:\", l1_loss_value)\n",
        "print(\"L2 loss:\", l2_loss_value)\n",
        "print(\"Binary cross entropy loss:\", binary_cross_entropy_value)\n",
        "print(\"Categorical cross entropy loss:\", categorical_cross_entropy_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmZIiak1IhOI",
        "outputId": "70b40d81-9019-4b95-d126-0b3c66a4e52f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 loss: 0.29999999999999993\n",
            "L2 loss: 0.029999999999999992\n",
            "Binary cross entropy loss: 0.31608154697347884\n",
            "Categorical cross entropy loss: 0.21072103131565256\n"
          ]
        }
      ]
    }
  ]
}